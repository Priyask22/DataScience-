{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66de6c29",
   "metadata": {},
   "source": [
    "# 19 march(FE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7324cb",
   "metadata": {},
   "source": [
    "Question 1: What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application\n",
    "\n",
    "Answer 1\n",
    "Min-Max scaling is a data normalization technique used to rescale the data to a fixed range, usually [0, 1]. It works by subtracting the minimum value of the feature from each data point and dividing the result by the range (the difference between the maximum and minimum values). Min-Max scaling is commonly used in data preprocessing to ensure that all features have the same scale and importance in the analysis. For example, consider a dataset of house prices, where the prices range from ₹50,000 to ₹1,000,000. Applying Min-Max scaling would transform the prices to a range of [0, 1], making it easier to compare the relative importance of other features such as the size of the house or the number of bedrooms.\n",
    "\n",
    "Question 2: What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "Answer 2\n",
    "The Unit Vector technique is a data normalization technique used to normalize vectors so that they have a magnitude of 1. It works by dividing each vector by its L2 norm (the square root of the sum of the squares of its components). Unlike Min-Max scaling, which rescales the data to a fixed range, Unit Vector scaling does not change the relative magnitudes of the features. Instead, it only ensures that the direction of the features is preserved. For example, consider a dataset of customer ratings on a scale from 1 to 10, where each rating represents a vector with 10 components. Applying Unit Vector scaling would normalize each vector so that its magnitude is 1, but the relative magnitudes of the components would remain the same.\n",
    "\n",
    "Question 3 : What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "Answer 3\n",
    "PCA (Principle Component Analysis) is a feature extraction technique used to reduce the dimensionality of the data while preserving as much of the original information as possible. It works by identifying a smaller number of uncorrelated variables, known as principal components, that can explain most of the variability in the data. PCA is commonly used in dimensionality reduction to simplify the analysis and reduce the computational complexity of machine learning algorithms. For example, consider a dataset of images with millions of pixels. Applying PCA to the dataset would identify a smaller number of principal components that can represent most of the important information in the images, making it easier to classify and compare images.\n",
    "\n",
    "Question 4 : What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "Answer 4\n",
    "PCA (Principal Component Analysis) is a type of feature extraction technique that can be used to reduce the dimensionality of the data while preserving most of the important information. Feature extraction refers to the process of identifying the most important features in a dataset and transforming the data into a lower-dimensional space. PCA can be used for feature extraction by identifying the most important principal components that explain most of the variability in the data and transforming the data into a lower-dimensional space defined by these components. This can reduce the complexity of the data and improve the efficiency of the analysis. For example, consider a dataset of customer reviews of a product with many features, such as quality, price, and customer service. Applying PCA to the dataset would identify the most important principal components that explain most of the variability in the data and transform the data into a lower-dimensional space defined by these components, making it easier to analyze and compare the reviews.\n",
    "\n",
    "Question 5 : You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "Answer 5\n",
    "To preprocess the data for the recommendation system, we would use Min-Max scaling to ensure that all features have the same scale and importance in the analysis. We would apply Min-Max scaling to the price, rating, and delivery time features to transform them to a fixed range, usually [0, 1]. For example, suppose the price feature ranges from \n",
    "25, the rating feature ranges from 1 to 5, and the delivery time feature ranges from 10 to 60 minutes. Applying Min-Max scaling to each feature would transform the ranges to [0, 1], making it easier to compare the relative importance of each feature in the analysis. After preprocessing, we could use the scaled features to build a recommendation system based on the customer preferences for each feature.\n",
    "\n",
    "Question 6 : You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "Answer 6\n",
    "to reduce the dimensionality of the dataset, we would first normalize the features to ensure that they are on the same scale. Then, we would apply PCA to identify the most important principal components that explain most of the variability in the data. We would choose the number of principal components to retain based on the amount of variance we want to preserve in the data. For example, we could choose to retain 90% of the variance in the data, which would allow us to reduce the number of features while preserving most of the important information. This would simplify the model and improve its efficiency.\n",
    "\n",
    "\n",
    "Question 7 : For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "Answer 7\n",
    "To perform Min-Max scaling on the dataset, we would use the formula:\n",
    "\n",
    "scaled_value = (value - min) / (max - min) * (max_range - min_range) + min_range\n",
    "\n",
    "where min = 1, max = 20, min_range = -1, max_range = 1. Applying the formula to each value in the dataset, we get:\n",
    "\n",
    "[(-1 + 1/4), (-1 + 3/4), (-1 + 7/4), (-1 + 11/4), (-1 + 15/4)]\n",
    "\n",
    "Simplifying, we get:\n",
    "\n",
    "[-3/4, -1/4, 1/2, 3/4, 1]\n",
    "\n",
    "Therefore, the transformed dataset ranges from -1 to 1.\n",
    "Question 8 : For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "Answer 8\n",
    "To perform feature extraction using PCA on the dataset, we would first normalize the features to ensure that they are on the same scale. Then, we would apply PCA to identify the most important principal components that explain most of the variability in the data. The number of principal components to retain would depend on the amount of variance we want to preserve in the data. A common approach is to retain enough principal components to explain at least 80% of the variance in the data. We would choose the number of principal components based on the cumulative explained variance ratio, which indicates the proportion of the total variance in the data that is explained by each principal component. For example, if the cumulative explained variance ratio reaches 80% after retaining the first three principal components, we would choose to retain those three principal components. This would simplify the dataset and improve the efficiency of the analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
