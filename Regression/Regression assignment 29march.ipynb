{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536d459f",
   "metadata": {},
   "source": [
    "# Regression 29 march"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17609e",
   "metadata": {},
   "source": [
    "Q1.What is lasso regression and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso Regression, also known as L1 regularization, is a linear regression technique that adds a penalty term to the cost function to encourage the model to have fewer non-zero coefficients. This encourages the model to select only the most important features and can help prevent overfitting.\n",
    "\n",
    "Compared to other regression techniques like Ridge Regression (L2 regularization) and Ordinary Least Squares (OLS), Lasso Regression differs in the penalty term it adds to the cost function. While Ridge Regression adds the squared sum of the coefficients, Lasso Regression adds the absolute sum of the coefficients. This results in a sparser model with fewer non-zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61f7c1",
   "metadata": {},
   "source": [
    "Q2.What is the main advantage of using lasso regression in feature selection?\n",
    "\n",
    "The main advantage of using Lasso Regression in feature selection is that it can automatically select the most important features and eliminate the irrelevant or redundant ones. This is particularly useful in high-dimensional data sets with many features, where selecting the right subset of features can improve the model's accuracy and interpretability.\n",
    "\n",
    "Lasso Regression achieves feature selection by shrinking the coefficients of the irrelevant or redundant features to zero. This is because the penalty term in the cost function encourages the model to have fewer non-zero coefficients. The resulting sparse model can improve model performance and reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b5028",
   "metadata": {},
   "source": [
    "Q3.How do yo interpret the coefficient of a lasso regression model?\n",
    "\n",
    "The coefficients of a Lasso Regression model can be interpreted as the impact of each feature on the predicted output. A positive coefficient means that the feature has a positive effect on the output, while a negative coefficient means that the feature has a negative effect on the output.\n",
    "\n",
    "However, it is important to note that the coefficients in a Lasso Regression model can be biased and difficult to interpret when the features are highly correlated. This is because Lasso Regression tends to select only one feature from a group of highly correlated features, leading to unstable and biased coefficient estimates.\n",
    "\n",
    "To address this issue, researchers have developed alternative techniques such as Elastic Net Regression, which combines both L1 and L2 regularization to achieve better balance between feature selection and coefficient stability.\n",
    "\n",
    "Q4.What are the tuning parameters that can be adjusted in Lasso regression and how do they affect the model's performance?\n",
    "\n",
    "The main tuning parameter in Lasso Regression is the regularization parameter, also known as the lambda parameter. This parameter controls the strength of the penalty term added to the cost function and determines the sparsity of the resulting model.\n",
    "\n",
    "A higher lambda value leads to a sparser model with fewer non-zero coefficients, which can help prevent overfitting and improve model interpretability. However, setting the lambda value too high can also lead to underfitting and decreased model performance. On the other hand, setting the lambda value too low can lead to overfitting and decreased model generalization.\n",
    "\n",
    "Another tuning parameter that can be adjusted in Lasso Regression is the intercept term, which represents the value of the predicted output when all the features have a value of zero. The intercept term can be adjusted to improve model accuracy and fit the data better.\n",
    "\n",
    "Q5.Can lasso regression model be used for non linear regression problems?If yes, how?\n",
    "\n",
    "Lasso Regression is a linear regression technique and is designed to model linear relationships between the features and the output. However, it can be extended to non-linear regression problems by including polynomial features or other non-linear transformations of the features.\n",
    "\n",
    "By including polynomial features, Lasso Regression can model non-linear relationships between the features and the output. However, adding too many polynomial features can lead to overfitting and decreased model performance. Therefore, it is important to carefully select the appropriate degree of polynomial features to balance model complexity and accuracy.\n",
    "\n",
    "Q6.What is the difference between  the ridge regression and lasso regression?\n",
    "\n",
    "Ridge Regression and Lasso Regression are two popular linear regression techniques that add a penalty term to the cost function to improve model performance and prevent overfitting.\n",
    "\n",
    "The main difference between Ridge Regression and Lasso Regression is the type of penalty term they use. Ridge Regression adds the squared sum of the coefficients (L2 regularization) to the cost function, while Lasso Regression adds the absolute sum of the coefficients (L1 regularization).\n",
    "\n",
    "As a result, Ridge Regression tends to produce models with smaller coefficients and can handle highly correlated features better than Lasso Regression. On the other hand, Lasso Regression tends to produce sparser models with fewer non-zero coefficients and can perform feature selection automatically.\n",
    "\n",
    "Q7.Can lasso regression handle multicollinearity in the input features ?If yes ,how?\n",
    "\n",
    "Lasso Regression can handle multicollinearity in the input features to some extent, but it may not perform as well as Ridge Regression in this scenario. Multicollinearity occurs when two or more input features are highly correlated with each other, which can lead to unstable and biased coefficient estimates in linear regression models.\n",
    "\n",
    "In Lasso Regression, the penalty term encourages the model to select only one feature from a group of highly correlated features, leading to a sparse model with fewer non-zero coefficients. However, this may not be sufficient to fully address multicollinearity, as it may not be clear which of the highly correlated features is the most important for the model.\n",
    "\n",
    "Ridge Regression, on the other hand, can handle multicollinearity more effectively by shrinking the coefficients of all the correlated features together. This helps to reduce the impact of multicollinearity on the model and can improve the stability of the coefficient estimates.\n",
    "\n",
    "Q8.How do you choose the optimal value of the regularisation parameter in Lasso regression?\n",
    "\n",
    "The optimal value of the regularization parameter in Lasso Regression can be selected using cross-validation techniques such as k-fold cross-validation or leave-one-out cross-validation. Cross-validation involves splitting the data set into training and validation sets, fitting the model on the training set, and evaluating its performance on the validation set. This process is repeated multiple times, with different subsets of the data used for training and validation, to obtain an estimate of the model's performance.\n",
    "\n",
    "The value of lambda that results in the best model performance on the validation set can then be selected as the optimal value. This process can be automated using tools like grid search or randomized search, which search for the optimal value of lambda over a range of possible values.\n",
    "\n",
    "It is important to note that the choice of the optimal value of lambda depends on the specific data set and the goals of the analysis. A larger lambda value leads to a sparser model with fewer non-zero coefficients, which can improve model interpretability but may decrease model performance. On the other hand, a smaller lambda value leads to a less sparse model with more non-zero coefficients, which can improve model performance but may decrease model interpretability. Therefore, the choice of lambda should be made with a balance between model interpretability and performance in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e9439",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1da0942b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563491a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978f9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cf94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42301b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a281b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b8279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf084b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a76c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72151377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
